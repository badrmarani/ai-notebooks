{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as f\n",
    "import torchvision\n",
    "\n",
    "from tqdm import trange\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "params = {\n",
    "    \"LR\": 3e-4,\n",
    "    \"N_BATCHS\": 32,\n",
    "    \"N_EPOCHS\": 10,\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Device type:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./datasets\", train=True, transform=transform, download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./datasets\", train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=params[\"N_BATCHS\"], shuffle=True,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=params[\"N_BATCHS\"], shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self,\n",
    "        latent_space_size=4,\n",
    "    ) -> None:\n",
    "        super(AE, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(5,5), padding=1, stride=2),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=(5,5), padding=1, stride=2),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ELU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128*6*6, latent_space_size),\n",
    "        )\n",
    "        self.deconv_0 = nn.Sequential(\n",
    "            nn.Linear(latent_space_size, 128*6*6),\n",
    "            # nn.BatchNorm1d(128*6*6),\n",
    "            nn.ELU(),\n",
    "        )\n",
    "        self.deconv_1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 46, kernel_size=(5,5), padding=1, stride=2),\n",
    "            nn.BatchNorm2d(46),\n",
    "            nn.ELU(),\n",
    "            nn.ConvTranspose2d(46, 1, kernel_size=(6,6), padding=1, stride=2),\n",
    "            nn.Sigmoid(), # the prediction probabilities of each class are computed independently of each other.\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), 1, 28, 28)\n",
    "        z = self.conv(x)\n",
    "        z = self.deconv_0(z).view(x.size(0), 128, 6, 6)\n",
    "        x_hat = self.deconv_1(z).view(x.size(0), 1, 28, 28)\n",
    "        return x_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ae = AE()\n",
    "model_ae= nn.DataParallel(model_ae)\n",
    "model_ae.to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch.optim.Adam(model_ae.parameters(), lr=params[\"LR\"])\n",
    "\n",
    "train_history, test_history = [], []\n",
    "\n",
    "for epoch in range(params[\"N_EPOCHS\"]):\n",
    "    train_loss = 0.\n",
    "    test_loss = 0.\n",
    "    for X, _ in train_loader:\n",
    "        X = X.to(device)\n",
    "        X_hat = model_ae(X)\n",
    "        loss = loss_fn(X_hat, X)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, _ in test_loader:\n",
    "            X = X.to(device)\n",
    "            X_hat = model_ae(X)\n",
    "            loss = loss_fn(X_hat, X)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)            \n",
    "    test_loss /= len(test_loader)\n",
    "    print(f\"{epoch+1}/{params['N_EPOCHS']} | train_loss = {train_loss:.6f}; eval_loss = {test_loss:.6f}\")\n",
    "    train_history.append(train_loss)\n",
    "    test_history.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = [random.choice(train_dataset.data) for _ in range(20)]\n",
    "RANDOM_SAMPLE = [random.choice(torch.randn(size=(140, 28, 28))) for _ in range(20)]\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.title(\"Input\")\n",
    "plt.imshow(np.concatenate(RANDOM_SAMPLE, axis=1))\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.title(\"Output : AE\")\n",
    "plt.imshow(np.concatenate([model_ae(x.to(device).float().view(1, 1, 28, 28)).detach().cpu().numpy().reshape((28,28)) for x in RANDOM_SAMPLE], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,16))\n",
    "plt.title(\"MSE loss\")\n",
    "plt.grid()\n",
    "plt.plot(train_history, label=\"train loss\")\n",
    "plt.plot(test_history, label=\"test loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self,\n",
    "        hid_size=200,\n",
    "        dist_size=10,\n",
    "        num_base_channels=1,\n",
    "        act_fn=nn.ReLU) -> None:\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(28*28, hid_size),\n",
    "            act_fn(),\n",
    "            nn.Linear(hid_size, hid_size),\n",
    "            act_fn(),\n",
    "            nn.Linear(hid_size, hid_size),\n",
    "            act_fn(),\n",
    "        )\n",
    "\n",
    "        self.mu, self.sigma = nn.Linear(hid_size, dist_size), nn.Linear(hid_size, dist_size)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(dist_size, hid_size),\n",
    "            act_fn(),\n",
    "            nn.Linear(hid_size, hid_size),\n",
    "            act_fn(),\n",
    "            nn.Linear(hid_size, 28*28),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> tuple:\n",
    "        num_batchs = x.size(0)\n",
    "        x = x.view(num_batchs, -1)\n",
    "        x = self.encoder(x)\n",
    "        mu, sigma = self.mu(x), self.sigma(x)\n",
    "        z = mu + sigma * torch.randn_like(sigma)\n",
    "        z = self.decoder(z).view(x.size(0), 1, 28, 28)\n",
    "        return z, mu, sigma\n",
    "\n",
    "class VAELoss(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(VAELoss, self).__init__()\n",
    "    \n",
    "    def forward(self, x, x_hat, mu, sigma):\n",
    "        kl_divergence = - torch.sum(1+torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2))\n",
    "        return nn.BCELoss()(x_hat, x) + kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_vae = VAE()\n",
    "model_vae= nn.DataParallel(model_vae)\n",
    "model_vae.to(device)\n",
    "optimizer = torch.optim.Adam(model_vae.parameters(), lr=params[\"LR\"])\n",
    "loss_fn = VAELoss()\n",
    "\n",
    "\n",
    "# x_hat, mu, sigma = model_vae(x:=torch.randn(size=(1,1, 28,28)).to(device))\n",
    "# print(x_hat.size())\n",
    "# print(loss_fn(x, x_hat, mu, sigma))\n",
    "\n",
    "train_history, test_history = [], []\n",
    "\n",
    "for epoch in range(params['N_EPOCHS']):\n",
    "    train_loss = 0.\n",
    "    test_loss = 0.\n",
    "    for x, _ in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        x = x.view(x.size(0), 1, 28, 28).to(device)\n",
    "        x_hat, mu, sigma = model_vae(x)\n",
    "        loss = loss_fn(x, x_hat, mu, sigma)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, _ in test_loader:\n",
    "            x = x.view(x.size(0), 1, 28, 28).to(device)\n",
    "            x_hat, mu, sigma = model_vae(x)\n",
    "            loss = loss_fn(x_hat, x, mu, sigma)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)            \n",
    "    test_loss /= len(test_loader)\n",
    "    print(f\"{epoch+1}/{params['N_EPOCHS']} | train_loss = {train_loss:.6f}; eval_loss = {test_loss:.6f}\")\n",
    "    train_history.append(train_loss)\n",
    "    test_history.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE = [random.choice(train_dataset.data) for _ in range(20)]\n",
    "# SAMPLE = [random.choice(torch.randn(size=(140, 28, 28))) for _ in range(20)]\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.title(\"Input\")\n",
    "plt.imshow(np.concatenate(SAMPLE, axis=1))\n",
    "\n",
    "plt.figure(figsize=(16,16))\n",
    "plt.title(\"Output : VAE\")\n",
    "plt.imshow(np.concatenate([model_vae(x.to(device).float().view(1, 1, 28, 28))[0].detach().cpu().numpy().reshape((28,28)) for x in SAMPLE], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(16,16))\n",
    "plt.title(\"BCE loss\")\n",
    "plt.grid()\n",
    "plt.plot(train_history, label=\"train loss\")\n",
    "plt.plot(test_history, label=\"test loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f0194299e911b6a22f0cd3d1c9a66c991d39f48b249be23f24104e40900e329"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
