{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "params = {\n",
    "    \"LR\": 1e-4,\n",
    "    \"N_BATCHS\": 32,\n",
    "    \"N_EPOCHS\": 10,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(\"Device type:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_autoencoder(nn.Module):\n",
    "    def __init__(self,\n",
    "        num_hid = 128,\n",
    "        act_fn = nn.ReLU,\n",
    "    ) -> None:\n",
    "        super(linear_autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(1*28*28, num_hid),\n",
    "            act_fn(),\n",
    "            nn.Linear(num_hid, 50),\n",
    "            act_fn(),\n",
    "            nn.Linear(50, 10),\n",
    "            act_fn(),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(10, 50),\n",
    "            act_fn(),\n",
    "            nn.Linear(50, num_hid),\n",
    "            act_fn(),\n",
    "            nn.Linear(num_hid, 1*28*28),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        self.latent = nn.Sequential(\n",
    "            nn.Linear(10, 10),\n",
    "            act_fn(),\n",
    "        )\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        X = X.view(X.shape[0], -1)\n",
    "        h = self.encoder(X)\n",
    "        h = self.latent(h)\n",
    "        X_hat = self.decoder(h)\n",
    "        return X_hat.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./datasets\", train=True, transform=transform, download=True\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./datasets\", train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=params[\"N_BATCHS\"], shuffle=True, num_workers=4, pin_memory=True\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset, batch_size=params[\"N_BATCHS\"], shuffle=True, num_workers=4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = linear_autoencoder()\n",
    "loss_fn = nn.MSELoss()\n",
    "opt = torch.optim.Adam(model.parameters(), lr=params[\"LR\"])\n",
    "\n",
    "train_history, test_history = [], []\n",
    "\n",
    "for epoch in range(params[\"N_EPOCHS\"]):\n",
    "    train_loss = 0.\n",
    "    test_loss = 0.\n",
    "    for X, _ in train_loader:\n",
    "        X_hat = model(X)\n",
    "\n",
    "        # print(\"x\", X.shape)\n",
    "        # print(\"xhat\", X_hat.shape)\n",
    "\n",
    "        loss = loss_fn(X_hat, X)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    for X, _ in train_loader:\n",
    "        X_hat = model(X)\n",
    "        loss = loss_fn(X_hat, X)\n",
    "        test_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)            \n",
    "    test_loss /= len(test_loader)\n",
    "    print(f\"> {epoch+1}/{params['N_EPOCHS']} | train_loss = {train_loss:.6f}; eval_loss = {test_loss:.6f}\")\n",
    "    train_history.append(train_loss)\n",
    "    test_history.append(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    fig, ax = plt.subplots(nrows=2, ncols=5,figsize=(20, 4))\n",
    "\n",
    "    for idx, col in enumerate(ax):\n",
    "        for idx2, row in enumerate(col):\n",
    "            X, y = train_dataset[idx2]\n",
    "            # torch.randint(high=len(train_dataset), size=(1,)).item()\n",
    "            \n",
    "            # X *= 255.\n",
    "            if idx == 0:\n",
    "                row.set_title(f\"{y}\")\n",
    "                row.imshow(X.view(28,28))\n",
    "            else:\n",
    "                row.set_title(f\"{y}\")\n",
    "                X_hat = model(X).view(28,28)\n",
    "                row.imshow(X_hat)\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(train_history, label=\"train\")\n",
    "    plt.plot(test_history, label=\"test\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c3d4d9970fc5c09c50d9318a9c7fbf4d0159e4b28833276f5678ee22bded273"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
